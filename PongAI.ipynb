{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1dff827-a906-43d3-8b3a-50e347ea2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab968658-1692-456c-8257-f0ca65f3db7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "2024-09-17 11:10:26.751 Python[4024:52921] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('Pong-v4', render_mode = \"human\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c4de3f6-87d7-4954-888b-46626b3dc4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.network(x), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e514c50-8dfa-4223-8e42-eee02d04edcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v4')\n",
    "input_size = np.prod(env.observation_space.shape)  # Flatten the observation space\n",
    "output_size = env.action_space.n  # Number of actions in Pong\n",
    "\n",
    "policy = PolicyNetwork(input_size, output_size)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0022b40-94db-4c75-9c0a-09b774d3d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(policy, state):\n",
    "    state = torch.FloatTensor(np.array(state).flatten()).unsqueeze(0)  # Convert to tensor and flatten state\n",
    "    probs = policy(state)\n",
    "    action = torch.multinomial(probs, 1).item()  # Sample action from probability distribution\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3d1fe7-e3d3-4d83-b539-dc3c20710761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy(policy, optimizer, trajectories, gamma=0.99):\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    \n",
    "    # Calculate discounted rewards for each trajectory\n",
    "    for trajectory in trajectories:\n",
    "        rewards = [step[2] for step in trajectory]\n",
    "        discounted_rewards = []\n",
    "        cumulative_reward = 0\n",
    "        for r in reversed(rewards):\n",
    "            cumulative_reward = r + gamma * cumulative_reward\n",
    "            discounted_rewards.insert(0, cumulative_reward)\n",
    "        \n",
    "        for (state, action, _, log_prob), R in zip(trajectory, discounted_rewards):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "    \n",
    "    # Perform optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy_loss).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7c34070-3482-4c05-8222-17f52d66d6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x13824 and 2592x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m trajectory \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_timesteps):\n\u001b[0;32m---> 10\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     13\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(policy(torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(state)\u001b[38;5;241m.\u001b[39mflatten())\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))[\u001b[38;5;241m0\u001b[39m, action])\n",
      "Cell \u001b[0;32mIn[16], line 41\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(policy, state)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(policy, state):\n\u001b[1;32m     40\u001b[0m     state \u001b[38;5;241m=\u001b[39m preprocess_state(state)  \u001b[38;5;66;03m# Preprocess state for CNN\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Sample action from probability distribution\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 21\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers(x)\n\u001b[0;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msoftmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x13824 and 2592x64)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Main training loop\n",
    "num_episodes = 1000\n",
    "max_timesteps = 10000\n",
    "gamma = 0.99\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state, info = env.reset()  # Unpack observation and additional info\n",
    "    trajectory = []\n",
    "    for t in range(max_timesteps):\n",
    "        action = select_action(policy, state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        log_prob = torch.log(policy(torch.FloatTensor(np.array(state).flatten()).unsqueeze(0))[0, action])\n",
    "        trajectory.append((state, action, reward, log_prob))\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # After each episode, update policy with the collected trajectory\n",
    "    train_policy(policy, optimizer, [trajectory], gamma=gamma)\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f'Episode {episode}: Finished training step')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dc97d8b-628e-46c1-88e9-76009e226adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a policy network with convolutional layers\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=8, stride=4),  # Convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),  # Second conv layer\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(32 * 9 * 9, 64),  # Adjust input size based on image processing\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "# Update the environment and policy initialization\n",
    "env = gym.make('Pong-v4')\n",
    "input_channels = env.observation_space.shape[2]  # Number of channels in the image (usually 3 for RGB)\n",
    "output_size = env.action_space.n  # Number of actions in Pong\n",
    "\n",
    "policy = PolicyNetwork(input_channels, output_size)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "# Adjust the state preprocessing for CNN input\n",
    "def preprocess_state(state):\n",
    "    state = np.transpose(state, (2, 0, 1))  # Change from HxWxC to CxHxW format for PyTorch\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)  # Add batch dimension\n",
    "    return state\n",
    "\n",
    "# Update the select_action function to handle image input\n",
    "def select_action(policy, state):\n",
    "    state = preprocess_state(state)  # Preprocess state for CNN\n",
    "    probs = policy(state)\n",
    "    action = torch.multinomial(probs, 1).item()  # Sample action from probability distribution\n",
    "    return action\n",
    "\n",
    "# Main training loop remains the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d62dc3-239d-429b-8cc3-f45fe5a40363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a policy network with convolutional layers\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=8, stride=4),  # Convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),  # Second conv layer\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Dynamically calculate the size of the flattened input from the conv layers\n",
    "        dummy_input = torch.zeros(1, input_channels, 210, 160)  # Example Pong input image (C, H, W)\n",
    "        conv_output_size = self.conv_layers(dummy_input).view(1, -1).size(1)\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, 64),  # Adjusted input size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the conv output\n",
    "        x = self.fc_layers(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "# Initialize environment and network\n",
    "env = gym.make('Pong-v4')\n",
    "input_channels = env.observation_space.shape[2]  # Number of channels in the image (usually 3 for RGB)\n",
    "output_size = env.action_space.n  # Number of actions in Pong\n",
    "\n",
    "policy = PolicyNetwork(input_channels, output_size)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "# Preprocess state to prepare it for CNN\n",
    "def preprocess_state(state):\n",
    "    state = np.transpose(state, (2, 0, 1))  # Change from HxWxC to CxHxW format for PyTorch\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)  # Add batch dimension\n",
    "    return state\n",
    "\n",
    "# Select action based on policy network's output\n",
    "def select_action(policy, state):\n",
    "    state = preprocess_state(state)  # Preprocess state for CNN\n",
    "    probs = policy(state)\n",
    "    action = torch.multinomial(probs, 1).item()  # Sample action from probability distribution\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a2c7f-bb3e-408b-84da-1d6de298319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "num_episodes = 1000\n",
    "max_timesteps = 500\n",
    "gamma = 0.99\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state, info = env.reset()  # Unpack observation and additional info\n",
    "    trajectory = []\n",
    "    for t in range(max_timesteps):\n",
    "        action = select_action(policy, state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        log_prob = torch.log(policy(preprocess_state(state))[0, action])\n",
    "        trajectory.append((state, action, reward, log_prob))\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # After each episode, update policy with the collected trajectory\n",
    "    train_policy(policy, optimizer, [trajectory], gamma=gamma)\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f'Episode {episode}: Finished training step')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2679397e-2846-458c-af4b-8048556b8875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a policy network with convolutional layers\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=8, stride=4),  # Convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),  # Second conv layer\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Dynamically calculate the size of the flattened input from the conv layers\n",
    "        dummy_input = torch.zeros(1, input_channels, 210, 160)  # Example Pong input image (C, H, W)\n",
    "        conv_output_size = self.conv_layers(dummy_input).view(1, -1).size(1)\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, 64),  # Adjusted input size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the conv output\n",
    "        x = self.fc_layers(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "# Preprocess state to prepare it for CNN\n",
    "def preprocess_state(state):\n",
    "    state = np.transpose(state, (2, 0, 1))  # Change from HxWxC to CxHxW format for PyTorch\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)  # Add batch dimension\n",
    "    return state\n",
    "\n",
    "# Select action based on policy network's output\n",
    "def select_action(policy, state):\n",
    "    state = preprocess_state(state)  # Preprocess state for CNN\n",
    "    probs = policy(state)\n",
    "    action = torch.multinomial(probs, 1).item()  # Sample action from probability distribution\n",
    "    return action\n",
    "\n",
    "# Function to compute discounted rewards\n",
    "def compute_discounted_rewards(trajectory, gamma):\n",
    "    rewards = [r for _, _, r, _ in trajectory]\n",
    "    discounted_rewards = []\n",
    "    cumulative_reward = 0\n",
    "    for reward in reversed(rewards):\n",
    "        cumulative_reward = reward + gamma * cumulative_reward\n",
    "        discounted_rewards.insert(0, cumulative_reward)  # Insert at the beginning to reverse the order\n",
    "    return discounted_rewards\n",
    "\n",
    "# Train the policy using trajectories\n",
    "def train_policy(policy, optimizer, trajectories, gamma=0.99):\n",
    "    policy.train()  # Set the policy network to training mode\n",
    "    \n",
    "    total_loss = 0\n",
    "    for trajectory in trajectories:\n",
    "        # Compute discounted rewards for each step in the trajectory\n",
    "        discounted_rewards = compute_discounted_rewards(trajectory, gamma)\n",
    "        discounted_rewards = torch.FloatTensor(discounted_rewards)\n",
    "        \n",
    "        # Normalize rewards to stabilize training\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "        \n",
    "        # Compute the policy loss (negative log prob * discounted reward)\n",
    "        policy_loss = []\n",
    "        for (_, action, reward, log_prob), Gt in zip(trajectory, discounted_rewards):\n",
    "            policy_loss.append(-log_prob * Gt)\n",
    "        \n",
    "        total_loss += torch.stack(policy_loss).sum()  # Combine all the losses for this trajectory\n",
    "    \n",
    "    # Backpropagate the loss\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "    total_loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update the policy network\n",
    "\n",
    "# Initialize environment and network\n",
    "env = gym.make('Pong-v4')\n",
    "input_channels = env.observation_space.shape[2]  # Number of channels in the image (usually 3 for RGB)\n",
    "output_size = env.action_space.n  # Number of actions in Pong\n",
    "\n",
    "policy = PolicyNetwork(input_channels, output_size)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adea50c0-309e-4541-9e95-f2179001c755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'select_action' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m trajectory \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_timesteps):\n\u001b[0;32m---> 10\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m(policy, state)\n\u001b[1;32m     11\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     13\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(policy(preprocess_state(state))[\u001b[38;5;241m0\u001b[39m, action])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'select_action' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_episodes = 1000\n",
    "max_timesteps = 500\n",
    "gamma = 0.99\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state, info = env.reset()  # Unpack observation and additional info\n",
    "    trajectory = []\n",
    "    for t in range(max_timesteps):\n",
    "        action = select_action(policy, state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        log_prob = torch.log(policy(preprocess_state(state))[0, action])\n",
    "        trajectory.append((state, action, reward, log_prob))\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # After each episode, update policy with the collected trajectory\n",
    "    train_policy(policy, optimizer, [trajectory], gamma=gamma)\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f'Episode {episode}: Finished training step')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "154480c8-4adb-4580-87dc-2e77de325ac3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the trained model after training (skip if already done)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mpolicy\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpong_policy_network.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the trained model after training (skip if already done)\n",
    "torch.save(policy.state_dict(), 'pong_policy_network.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af6c2334-8ed7-46b0-9d09-175f4061a084",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PolicyNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mPolicyNetwork\u001b[49m(input_channels, output_size)\n\u001b[1;32m      3\u001b[0m policy\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpong_policy_network.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m policy\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the policy to evaluation mode\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PolicyNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "policy = PolicyNetwork(input_channels, output_size)\n",
    "policy.load_state_dict(torch.load('pong_policy_network.pth'))\n",
    "policy.eval()  # Set the policy to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "934aff88-a2d3-4e49-8b15-4f0dd6ab266a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kpradjinata/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:289: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n",
      "2024-09-16 13:08:01.753 Python[66014:4414158] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPong-v4\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Visualize the performance of the trained policy\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Play 5 episodes to visualize performance\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(policy, env, num_episodes)\u001b[0m\n\u001b[1;32m     13\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()  \u001b[38;5;66;03m# Render the game (visualize the performance)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m action \u001b[38;5;241m=\u001b[39m select_action(policy, state)  \u001b[38;5;66;03m# Use the trained policy to select an action\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Take the action in the environment\u001b[39;00m\n\u001b[1;32m     16\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# Accumulate the reward\u001b[39;00m\n\u001b[1;32m     17\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state  \u001b[38;5;66;03m# Move to the next state\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/ale_py/env/gym.py:256\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[0;34m(self, action_ind)\u001b[0m\n\u001b[1;32m    254\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[0;32m--> 256\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43male\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    258\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Function to visualize the trained policy\n",
    "def evaluate_policy(policy, env, num_episodes=5):\n",
    "    policy.eval()  # Set the policy to evaluation mode (no gradients calculated)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()  # Reset the environment\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            env.render()  # Render the game (visualize the performance)\n",
    "            action = select_action(policy, state)  # Use the trained policy to select an action\n",
    "            next_state, reward, done, _, _ = env.step(action)  # Take the action in the environment\n",
    "            total_reward += reward  # Accumulate the reward\n",
    "            state = next_state  # Move to the next state\n",
    "            time.sleep(0.03)  # Slow down the rendering to make it viewable\n",
    "\n",
    "        print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# Initialize the environment with render_mode='human' to visualize\n",
    "env = gym.make('Pong-v4', render_mode='human')\n",
    "\n",
    "# Visualize the performance of the trained policy\n",
    "evaluate_policy(policy, env, num_episodes=5)  # Play 5 episodes to visualize performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8524f007-ca58-48da-b451-ed4f6db66c07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: -21.0\n",
      "Episode 2: Total Reward: -21.0\n",
      "Episode 3: Total Reward: -21.0\n",
      "Episode 4: Total Reward: -21.0\n",
      "Episode 5: Total Reward: -21.0\n",
      "Episode 6: Total Reward: -21.0\n",
      "Episode 7: Total Reward: -21.0\n",
      "Episode 8: Total Reward: -21.0\n",
      "Episode 9: Total Reward: -21.0\n",
      "Episode 10: Total Reward: -21.0\n",
      "Episode 11: Total Reward: -21.0\n",
      "Episode 12: Total Reward: -21.0\n",
      "Episode 13: Total Reward: -21.0\n",
      "Episode 14: Total Reward: -21.0\n",
      "Episode 15: Total Reward: -21.0\n",
      "Episode 16: Total Reward: -21.0\n",
      "Episode 17: Total Reward: -21.0\n",
      "Episode 18: Total Reward: -21.0\n",
      "Episode 19: Total Reward: -21.0\n",
      "Episode 20: Total Reward: -21.0\n",
      "Episode 21: Total Reward: -21.0\n",
      "Episode 22: Total Reward: -21.0\n",
      "Episode 23: Total Reward: -21.0\n",
      "Episode 24: Total Reward: -21.0\n",
      "Episode 25: Total Reward: -21.0\n",
      "Episode 26: Total Reward: -21.0\n",
      "Episode 27: Total Reward: -21.0\n",
      "Episode 28: Total Reward: -21.0\n",
      "Episode 29: Total Reward: -21.0\n",
      "Episode 30: Total Reward: -21.0\n",
      "Episode 31: Total Reward: -21.0\n",
      "Episode 32: Total Reward: -21.0\n",
      "Episode 33: Total Reward: -21.0\n",
      "Episode 34: Total Reward: -21.0\n",
      "Episode 35: Total Reward: -21.0\n",
      "Episode 36: Total Reward: -21.0\n",
      "Episode 37: Total Reward: -21.0\n",
      "Episode 38: Total Reward: -21.0\n",
      "Episode 39: Total Reward: -21.0\n",
      "Episode 40: Total Reward: -21.0\n",
      "Episode 41: Total Reward: -21.0\n",
      "Episode 42: Total Reward: -21.0\n",
      "Episode 43: Total Reward: -21.0\n",
      "Episode 44: Total Reward: -21.0\n",
      "Episode 45: Total Reward: -21.0\n",
      "Episode 46: Total Reward: -21.0\n",
      "Episode 47: Total Reward: -21.0\n",
      "Episode 48: Total Reward: -21.0\n",
      "Episode 49: Total Reward: -21.0\n",
      "Episode 50: Total Reward: -21.0\n",
      "Episode 51: Total Reward: -21.0\n",
      "Episode 52: Total Reward: -21.0\n",
      "Episode 53: Total Reward: -21.0\n",
      "Episode 54: Total Reward: -21.0\n",
      "Episode 55: Total Reward: -21.0\n",
      "Episode 56: Total Reward: -21.0\n",
      "Episode 57: Total Reward: -21.0\n",
      "Episode 58: Total Reward: -21.0\n",
      "Episode 59: Total Reward: -21.0\n",
      "Episode 60: Total Reward: -21.0\n",
      "Episode 61: Total Reward: -21.0\n",
      "Episode 62: Total Reward: -21.0\n",
      "Episode 63: Total Reward: -21.0\n",
      "Episode 64: Total Reward: -21.0\n",
      "Episode 65: Total Reward: -21.0\n",
      "Episode 66: Total Reward: -21.0\n",
      "Episode 67: Total Reward: -21.0\n",
      "Episode 68: Total Reward: -21.0\n",
      "Episode 69: Total Reward: -21.0\n",
      "Episode 70: Total Reward: -21.0\n",
      "Episode 71: Total Reward: -21.0\n",
      "Episode 72: Total Reward: -21.0\n",
      "Episode 73: Total Reward: -21.0\n",
      "Episode 74: Total Reward: -21.0\n",
      "Episode 75: Total Reward: -21.0\n",
      "Episode 76: Total Reward: -21.0\n",
      "Episode 77: Total Reward: -21.0\n",
      "Episode 78: Total Reward: -21.0\n",
      "Episode 79: Total Reward: -21.0\n",
      "Episode 80: Total Reward: -21.0\n",
      "Episode 81: Total Reward: -21.0\n",
      "Episode 82: Total Reward: -21.0\n",
      "Episode 83: Total Reward: -21.0\n",
      "Episode 84: Total Reward: -21.0\n",
      "Episode 85: Total Reward: -21.0\n",
      "Episode 86: Total Reward: -21.0\n",
      "Episode 87: Total Reward: -21.0\n",
      "Episode 88: Total Reward: -21.0\n",
      "Episode 89: Total Reward: -21.0\n",
      "Episode 90: Total Reward: -21.0\n",
      "Episode 91: Total Reward: -21.0\n",
      "Episode 92: Total Reward: -21.0\n",
      "Episode 93: Total Reward: -21.0\n",
      "Episode 94: Total Reward: -21.0\n",
      "Episode 95: Total Reward: -21.0\n",
      "Episode 96: Total Reward: -21.0\n",
      "Episode 97: Total Reward: -21.0\n",
      "Episode 98: Total Reward: -21.0\n",
      "Episode 99: Total Reward: -21.0\n",
      "Episode 100: Total Reward: -21.0\n",
      "Episode 101: Total Reward: -21.0\n",
      "Episode 102: Total Reward: -21.0\n",
      "Episode 103: Total Reward: -21.0\n",
      "Episode 104: Total Reward: -21.0\n",
      "Episode 105: Total Reward: -21.0\n",
      "Episode 106: Total Reward: -21.0\n",
      "Episode 107: Total Reward: -21.0\n",
      "Episode 108: Total Reward: -21.0\n",
      "Episode 109: Total Reward: -21.0\n",
      "Episode 110: Total Reward: -21.0\n",
      "Episode 111: Total Reward: -21.0\n",
      "Episode 112: Total Reward: -21.0\n",
      "Episode 113: Total Reward: -21.0\n",
      "Episode 114: Total Reward: -21.0\n",
      "Episode 115: Total Reward: -21.0\n",
      "Episode 116: Total Reward: -21.0\n",
      "Episode 117: Total Reward: -21.0\n",
      "Episode 118: Total Reward: -21.0\n",
      "Episode 119: Total Reward: -21.0\n",
      "Episode 120: Total Reward: -21.0\n",
      "Episode 121: Total Reward: -21.0\n",
      "Episode 122: Total Reward: -21.0\n",
      "Episode 123: Total Reward: -21.0\n",
      "Episode 124: Total Reward: -21.0\n",
      "Episode 125: Total Reward: -21.0\n",
      "Episode 126: Total Reward: -21.0\n",
      "Episode 127: Total Reward: -21.0\n",
      "Episode 128: Total Reward: -21.0\n",
      "Episode 129: Total Reward: -21.0\n",
      "Episode 130: Total Reward: -21.0\n",
      "Episode 131: Total Reward: -21.0\n",
      "Episode 132: Total Reward: -21.0\n",
      "Episode 133: Total Reward: -21.0\n",
      "Episode 134: Total Reward: -21.0\n",
      "Episode 135: Total Reward: -21.0\n",
      "Episode 136: Total Reward: -21.0\n",
      "Episode 137: Total Reward: -21.0\n",
      "Episode 138: Total Reward: -21.0\n",
      "Episode 139: Total Reward: -21.0\n",
      "Episode 140: Total Reward: -21.0\n",
      "Episode 141: Total Reward: -21.0\n",
      "Episode 142: Total Reward: -21.0\n",
      "Episode 143: Total Reward: -21.0\n",
      "Episode 144: Total Reward: -21.0\n",
      "Episode 145: Total Reward: -21.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Evaluate the policy and collect the total rewards\u001b[39;00m\n\u001b[1;32m     29\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Define how many episodes you want to evaluate\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Plot the rewards per episode\u001b[39;00m\n\u001b[1;32m     33\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_episodes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), rewards)\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(policy, env, num_episodes)\u001b[0m\n\u001b[1;32m     11\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 14\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use the trained policy to select an action\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# Take the action in the environment\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# Accumulate the reward\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(policy, state)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(policy, state):\n\u001b[1;32m     43\u001b[0m     state \u001b[38;5;241m=\u001b[39m preprocess_state(state)  \u001b[38;5;66;03m# Preprocess state for CNN\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Sample action from probability distribution\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the conv output\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_layers(x)\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/OpenAIGym/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to evaluate and track rewards\n",
    "def evaluate_policy(policy, env, num_episodes=10):\n",
    "    policy.eval()  # Set the policy to evaluation mode (no gradients are calculated)\n",
    "    rewards_per_episode = []  # To store total rewards per episode\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()  # Reset the environment\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(policy, state)  # Use the trained policy to select an action\n",
    "            next_state, reward, done, _, _ = env.step(action)  # Take the action in the environment\n",
    "            total_reward += reward  # Accumulate the reward\n",
    "            state = next_state  # Move to the next state\n",
    "\n",
    "        rewards_per_episode.append(total_reward)  # Store the total reward for this episode\n",
    "        print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    return rewards_per_episode\n",
    "\n",
    "# Initialize the environment with render_mode='human' if you want to visualize during evaluation\n",
    "env = gym.make('Pong-v4')\n",
    "\n",
    "# Evaluate the policy and collect the total rewards\n",
    "num_episodes = 1000  # Define how many episodes you want to evaluate\n",
    "rewards = evaluate_policy(policy, env, num_episodes)\n",
    "\n",
    "# Plot the rewards per episode\n",
    "plt.plot(range(1, num_episodes + 1), rewards)\n",
    "plt.title('Total Rewards per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b078e1-ed0e-4d33-91d1-33b18ef6feec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
