{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1716360f-a481-415b-8612-e27a7e03d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef8cb9f3-283a-4865-8045-202c55ff4c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the Pong environment\n",
    "env = gym.make('Pong-v4')\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Initial exploration rate\n",
    "epsilon_min = 0.01  # Minimum exploration rate\n",
    "epsilon_decay = 0.98  # Faster decay rate for epsilon\n",
    "learning_rate = 0.00025  # Learning rate\n",
    "batch_size = 32  # Batch size for experience replay\n",
    "target_update = 500  # Update target network every x steps (less frequent)\n",
    "memory_size = 5000  # Replay buffer size (smaller for quick demo)\n",
    "num_episodes = 25  # Reduced number of training episodes\n",
    "\n",
    "# gamma = 0.99  # Discount factor\n",
    "# epsilon = 1.0  # Initial exploration rate\n",
    "# epsilon_min = 0.01  # Minimum exploration rate\n",
    "# epsilon_decay = 0.995  # Epsilon decay rate\n",
    "# learning_rate = 0.00025  # Learning rate\n",
    "# batch_size = 32  # Batch size for experience replay\n",
    "# target_update = 1000  # Update target network every x steps\n",
    "# memory_size = 10000  # Replay buffer size\n",
    "# num_episodes = 100  # Number of training episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec6964-d7d2-468a-9202-d6fadcc2840c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166bf222-aa98-486e-a665-e00c5e2c1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1)  # This is the missing conv3 layer\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 256)  # Adjust this based on the output size of conv3\n",
    "        self.fc2 = nn.Linear(256, env.action_space.n)\n",
    "        \n",
    "        # self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        # self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        # self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # # Update this based on the output size after convolutions\n",
    "        # self.fc1 = nn.Linear(64 * 6 * 6, 512)  # Adjusted to 2304 input size\n",
    "        # self.fc2 = nn.Linear(512, env.action_space.n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ab6f22-0a78-49a6-ae7a-09de68888eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks and optimizer\n",
    "policy_net = DQN().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "target_net = DQN().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Experience Replay Memory\n",
    "memory = deque(maxlen=memory_size)\n",
    "\n",
    "# Function to process state (convert to grayscale and resize)\n",
    "def preprocess_state(state):\n",
    "    state = np.array(state)  # Ensure state is a NumPy array\n",
    "    if len(state.shape) == 3:  # Check if the state is 3D (height, width, channels)\n",
    "        state = np.mean(state, axis=2).astype(np.uint8)  # Convert to grayscale\n",
    "    elif len(state.shape) == 1:  # If the state is 1D, reshape and process it\n",
    "        state = np.array(state).reshape(210, 160, 3)  # Reshape to match Pong frame dimensions\n",
    "        state = np.mean(state, axis=2).astype(np.uint8)  # Convert to grayscale\n",
    "    \n",
    "    state = state[35:195]  # Crop\n",
    "    state = state[::2, ::2]  # Downsample by factor of 2\n",
    "    return state\n",
    "\n",
    "# Function to select an action using epsilon-greedy policy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "            q_values = policy_net(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "# Function to store experience in replay memory\n",
    "def store_experience(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "# Function to perform experience replay and train the DQN\n",
    "def replay():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.tensor(states, dtype=torch.float32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    actions = torch.tensor(actions).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    next_q_values = target_net(next_states).max(1)[0]\n",
    "    target_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
    "\n",
    "    loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ed9382-215e-4746-8152-a6250656fc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after conv layers: torch.Size([1, 32, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "def get_conv_output_size():\n",
    "    with torch.no_grad():\n",
    "        dummy_input = torch.zeros(1, 4, 80, 80)  # (batch_size, num_frames, height, width)\n",
    "        conv_out = policy_net.conv1(dummy_input)\n",
    "        conv_out = policy_net.conv2(conv_out)\n",
    "        print(f\"Output shape after conv layers: {conv_out.shape}\")\n",
    "\n",
    "get_conv_output_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5bfd16f-106f-46a1-99ae-35c21b0b74ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|                                   | 0/25 [00:00<?, ?it/s]/var/folders/kh/kdb6f8j523s5fh4syx6bp5bh0000gn/T/ipykernel_4593/1924133110.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  states = torch.tensor(states, dtype=torch.float32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
      "Training Progress:   4%|▉                       | 1/25 [03:15<1:18:17, 195.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|█▉                      | 2/25 [07:11<1:23:57, 219.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: -19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  12%|██▉                     | 3/25 [10:27<1:16:30, 208.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2, Total Reward: -20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  16%|███▊                    | 4/25 [13:55<1:12:53, 208.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3, Total Reward: -20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|████▊                   | 5/25 [16:49<1:05:22, 196.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4, Total Reward: -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  24%|█████▊                  | 6/25 [20:01<1:01:40, 194.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5, Total Reward: -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  28%|███████▎                  | 7/25 [23:28<59:35, 198.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6, Total Reward: -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  32%|███████▋                | 8/25 [27:42<1:01:14, 216.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 7, Total Reward: -19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  36%|████████▋               | 9/25 [32:14<1:02:21, 233.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 8, Total Reward: -17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|██████████               | 10/25 [35:33<55:45, 223.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 9, Total Reward: -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  44%|███████████              | 11/25 [38:43<49:40, 212.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, Total Reward: -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  48%|████████████             | 12/25 [42:41<47:47, 220.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 11, Total Reward: -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  52%|█████████████            | 13/25 [46:09<43:22, 216.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 12, Total Reward: -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  56%|██████████████           | 14/25 [49:32<38:58, 212.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 13, Total Reward: -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|███████████████          | 15/25 [52:49<34:39, 207.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 14, Total Reward: -20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  64%|████████████████         | 16/25 [55:55<30:11, 201.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 15, Total Reward: -20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  68%|█████████████████        | 17/25 [59:25<27:11, 203.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 16, Total Reward: -20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  72%|████████████████▌      | 18/25 [1:02:28<23:02, 197.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 17, Total Reward: -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  76%|█████████████████▍     | 19/25 [1:05:59<20:09, 201.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 18, Total Reward: -20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|██████████████████▍    | 20/25 [1:09:48<17:29, 209.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 19, Total Reward: -20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  84%|███████████████████▎   | 21/25 [1:12:41<13:15, 198.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, Total Reward: -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  88%|████████████████████▏  | 22/25 [1:15:35<09:33, 191.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 21, Total Reward: -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  92%|█████████████████████▏ | 23/25 [1:19:04<06:33, 196.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 22, Total Reward: -19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  96%|██████████████████████ | 24/25 [1:22:21<03:16, 196.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 23, Total Reward: -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|███████████████████████| 25/25 [1:26:18<00:00, 207.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 24, Total Reward: -19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Wrap the episode loop in tqdm for a progress bar\n",
    "for episode in tqdm(range(num_episodes), desc=\"Training Progress\"):\n",
    "    state, _ = env.reset()  # Adjust to capture the state correctly if reset returns a tuple\n",
    "    state = preprocess_state(state)\n",
    "    state = np.stack([state] * 4, axis=0)  # Stack 4 frames to create state\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = select_action(state, epsilon)\n",
    "\n",
    "        # Unpack 5 values returned from env.step()\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = np.stack([next_state] * 4, axis=0)  # Stack 4 frames\n",
    "        store_experience(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        replay()\n",
    "\n",
    "        # Handle both 'done' and 'truncated' conditions\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "\n",
    "    tqdm.write(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212d5fa2-002d-42d3-92ec-d87a245a80ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):  \u001b[38;5;66;03m# Evaluate for 5 episodes\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     state, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 10\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_state\u001b[49m(state)\n\u001b[1;32m     11\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([state] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_state' is not defined"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Set up the Pong environment with render_mode='human'\n",
    "env = gym.make('Pong-v4', render_mode='human')\n",
    "\n",
    "epsilon = 0  # Disable exploration to purely exploit the learned policy\n",
    "\n",
    "for episode in range(5):  # Evaluate for 5 episodes\n",
    "    state, _ = env.reset()\n",
    "    state = preprocess_state(state)\n",
    "    state = np.stack([state] * 4, axis=0)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = np.stack([next_state] * 4, axis=0)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        env.render()  # This will now work correctly with mode='human'\n",
    "\n",
    "        if done or truncated:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830f3b4-4f88-42b9-b5b0-2a61766c7ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
